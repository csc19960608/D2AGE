[param]

############################################
# training data dictory
############################################
# main work dir
root_dir = D:/dataset/dataset
# the name of one dataset, such as linkedin, facebook
dataset_name = facebook
# number of labels for each dataset, such as 10,100,1000
suffix = 100
# relatin name, such as classmateï¼Œfamily
class_name = classmate
# the index of the dataset file
index = 1

############################################
# paths for some prepared data
############################################
# words embeddings path
wordsEmbeddings_path = %(root_dir)s/%(dataset_name)s/nodesFeatures
# sub-paths save file
subpaths_file = %(root_dir)s/%(dataset_name)s/subpathsSaveFile
# DAGs save files
subgraphSaveFile = %(root_dir)s/%(dataset_name)s/subgraphSaveFile

############################################
# experiment parameters - do not need to change frequently
############################################
# the max length for sub-paths
maxlen_subpaths = 1000
# the size of words vocabulary
wordsSize = 10000000
# Sequence longer than this get ignored 
maxlen = 1000
# use a batch for training. This is the size of this batch.
batch_size = 10
# if need shuffle for training
is_shuffle_for_batch = True
# the frequences for display
dispFreq = 5
# the frequences for saving the parameters
saveFreq = 5
# the path for saving parameters. It is generated by main_dir, dataset_name, suffix, class_name and index. It will be generated in the code.
saveto = 
# the top num to predict
top_num = 10

############################################
# experiment parameters 
############################################
# learning rate
lrate = 0.0001
# dimension of words embeddings
word_dimension = 10
# the dimension of paths embeddings
dimension = 18

# discount parameter alpha
discount_alpha = 0.3
# discount parameter beta
discount_beta = 0.3
# the output way of DAG-LSTM. There are three ways, "h" only uses the last output h as the output of lstm for one path; "mean-pooling" uses the mean-pooling of all hi as the output of lstm for one path; "max-pooling" uses the max-pooling of all hi as the output of lstm for one path.
h_output_method = max-pooling
# loss function, we use sigmoid
objective_function_method = sigmoid
# the parameter in loss function, mu
objective_function_param = 0.5
# the max epochs for training
max_epochs = 50
# decay parameter lambda
decay = 0.0001

############################################
# DAG generation parameters
############################################
# the number of generating DAGs between (q,v)
subgraphNum = 0
# if subgraphNum = 0, then use this proportion to generate the DAGs. subgraphNum=proportion * #(subpaths)
proportion = 0.8