#encoding=utf-8
'''
@author: Liu Zemin
Functions and Application : 
'''


import ConfigParser
import string, os, sys
import proxEmbedBySubgraphs
import proxEmbedBySubgraphProcessAndAssess
import time

if __name__=='__main__':
    
    cf = ConfigParser.SafeConfigParser()
    cf.read("pythonParamsConfig")
    
    main_dir=cf.get("param", "root_dir") # main work dir
    dataset_name=cf.get("param", "dataset_name") # dataset name, such as facebook
    suffix=cf.get("param", "suffix") # number of labels for each dataset, such as 10,100,1000
    class_name=cf.get("param", "class_name") # relatin name, such as classmateï¼Œfamily
    index=cf.get("param", "index") # the index of the dataset file
    
    trainingDataFile=os.path.join(main_dir+'/',dataset_name+'.splits','train.'+suffix,'train_'+class_name+'_'+index) # the full path of training data file. This path will be generated by main_dir, dataset_name, suffix, class_name and index.
    
    wordsEmbeddings=None # the file path of words embeddings
    wordsEmbeddings_path=cf.get("param", "wordsEmbeddings_path") # dimension of words embeddings
    subpaths_map=None # the map to save all the subpaths
    subpaths_file=cf.get("param", "subpaths_file") # the file to save all the subpaths
    subgraphSaveFile=cf.get("param", "subgraphSaveFile") # the file to save all the DAGs
    
    maxlen_subpaths=cf.getint("param", "maxlen_subpaths")  # the max length for sub-paths
    wordsSize=cf.getint("param", "wordsSize") # the max size of words vocabulary
    maxlen=cf.getint("param", "maxlen") # Sequence longer than this get ignored 
    batch_size=cf.getint("param", "batch_size") # use a batch for training. This is the size of this batch.
    is_shuffle_for_batch=cf.getboolean("param", "is_shuffle_for_batch") # if need shuffle for training
    
    dispFreq=cf.getint("param", "dispFreq") # the frequences for display
    saveFreq=cf.getint("param", "saveFreq") # the frequences for saving the parameters
    saveto=os.path.join(main_dir+'/',dataset_name+'.trainModels','train.'+suffix,'train_'+class_name+'_'+index+'.npz') # the path for saving parameters. It is generated by main_dir, dataset_name, suffix, class_name and index.
    
    lrate=cf.getfloat("param", "lrate") # learning rate
    word_dimension=cf.getint("param", "word_dimension") # dimension of words embeddings
    dimension=cf.getint("param", "dimension") # the dimension of paths embeddings
    discount_alpha=cf.getfloat("param", "discount_alpha") # parameter alpha
    discount_beta=cf.getfloat("param", "discount_beta") # parameter beta
    h_output_method=cf.get("param", "h_output_method") # the way of output for each DAG, we use the hidden state of the end node in a DAG as its output
    objective_function_method=cf.get("param", "objective_function_method") # the objective function, here we use sigmoid
    objective_function_param=cf.getfloat("param", "objective_function_param") # the parameter mu for sigmoid
    max_epochs=cf.getint("param", "max_epochs") # the max epoches for training 
    
    decay=cf.getfloat("param", "decay") # the decay parameter lambda
    
    test_data_file=os.path.join(main_dir+'/',dataset_name+'.splits','test','test_'+class_name+'_'+index) # the file of test data
    top_num=cf.getint("param", "top_num") # the top num to predict
    ideal_data_file=os.path.join(main_dir+'/',dataset_name+'.splits','ideal','ideal_'+class_name+'_'+index) # the file of ground truth
    
    # training
    proxEmbedBySubgraphs.proxEmbedBySubgraphs(
                        trainingDataFile, 
                        wordsEmbeddings, 
                        wordsEmbeddings_path, 
                        subpaths_map, 
                        subpaths_file, 
                        subgraphSaveFile, 
                        maxlen_subpaths, 
                        wordsSize, 
                        maxlen, 
                        batch_size, 
                        is_shuffle_for_batch, 
                        dispFreq, 
                        saveFreq, 
                        saveto, 
                        lrate, 
                        word_dimension, 
                        dimension, 
                        discount_alpha,
                        discount_beta,
                        h_output_method,
                        objective_function_method, 
                        objective_function_param, 
                        max_epochs, 
                        decay)
    
    time.sleep(5) # sleep
    
    
    print '------------------------------------------------------------------------------'
    print 'Start to generate process model..........'
    start_time = time.time() 
    print 'This time ==',time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(start_time))
    # get the model for process
    func=proxEmbedBySubgraphProcessAndAssess.get_proxEmbedBySubgraphModel(
                                                                     saveto, 
                                                                     word_dimension, 
                                                                     dimension, 
                                                                    discount_alpha,
                                                                    discount_beta,
                                                                     h_output_method)
     
    print 'Start to process and evaluate the model..........'
    start_time = time.time() 
    print 'This time ==',time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(start_time))
    # calculate the results
    MAP, NDCG=proxEmbedBySubgraphProcessAndAssess.compute_proxEmbedBySubgraph(
                        wordsEmbeddings, 
                        wordsEmbeddings_path, 
                        word_dimension, 
                        dimension, 
                        wordsSize, 
                        subpaths_map, 
                        subpaths_file, 
                        subgraphSaveFile, 
                        maxlen_subpaths, 
                        maxlen, 
                        test_data_file, 
                        top_num, 
                        ideal_data_file, 
                        func)
    print '------------------------------------------------------------------------------'
    start_time = time.time() 
    print 'This time ==',time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(start_time))
    print 'MAP =', MAP
    print 'NDCG =', NDCG